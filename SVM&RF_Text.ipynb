{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/notmanan/Depression-Detection-Through-Multi-Modal-Data/blob/master/SVM%26RF_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"ZGgide74kahW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681409737335,"user_tz":-330,"elapsed":29338,"user":{"displayName":"Deepression SLP","userId":"18338550655322334499"}},"outputId":"85c9eaee-7bbd-40dd-adcf-8cef5d03c164"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import numpy as np\n","import pandas as pd\n","from gensim.models.keyedvectors import KeyedVectors\n","from nltk.corpus import stopwords\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_log_error\n","\n","\n","import os\n","os.chdir('drive/My Drive/MCA Dataset')\n","\n","import nltk\n","nltk.download('stopwords')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Isu6Ypp1mEUO"},"source":["# Preparing Data"]},{"cell_type":"code","metadata":{"id":"Ec8fm3lcg_2y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681409851485,"user_tz":-330,"elapsed":114168,"user":{"displayName":"Deepression SLP","userId":"18338550655322334499"}},"outputId":"036a20ae-e564-4275-b58d-3967573e9da9"},"source":["dataset1 = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/devdata.csv',delimiter=',',encoding='utf-8'))[:, 0:4]\n","dataset2 = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/testdata.csv',delimiter=',',encoding='utf-8'))[:, 0:4]\n","dataset3 = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/traindata.csv',delimiter=',',encoding='utf-8'))[:, 0:4]\n","\n","dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n","pos = []\n","neg = []\n","\n","countPos = 0\n","\n","def checkPosNeg(dataset, index):\n","    for i in range(0, len(dataset)):\n","        if(dataset[i][0] == index):\n","            return dataset[i][2]\n","    return 0\n","\n","for i in range(len(dataset)):\n","    if dataset[i][0]<=310 and dataset[i][0]>=300:\n","        if(dataset[i][1] == 1):\n","            neg.append(dataset[i][0])\n","        else:\n","            pos.append(dataset[i][0])\n","    elif dataset[i][0]<=355 and dataset[i][0]>=345:\n","        if(dataset[i][1] == 1):\n","            neg.append(dataset[i][0])\n","        else:\n","            pos.append(dataset[i][0])\n","            \n","pos = np.array(pos)\n","neg = np.array(neg)\n","\n","Data = []\n","Y = []\n","\n","Data_test = []\n","Y_test = []\n","index = -1\n","for i in range(0, len(dataset3)):\n","    val = checkPosNeg(dataset, dataset3[i][0])\n","    if(val == 0 and countPos>37):\n","        continue\n","    Y.append(val)\n","    index+=1\n","    if(Y[index] == 0):\n","        countPos+=1\n","    fileName = \"/content/drive/My Drive/MCA Dataset/303_P/\" + str(int(dataset3[i][0])) + \"_TRANSCRIPT.csv\"\n","    Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n","\n","countPos = 0\n","for i in range(0, len(dataset1)):\n","    val = checkPosNeg(dataset, dataset1[i][0])\n","    if(val == 0 and countPos>37):\n","        continue\n","    Y.append(val)\n","    index+=1\n","    if(Y[index] == 0):\n","        countPos+=1\n","    # if(val == 0 and countPos):\n","    #     continue\n","    # Y.append(val)\n","    fileName = \"/content/drive/My Drive/MCA Dataset/302_P/\" + str(int(dataset1[i][0])) + \"_TRANSCRIPT.csv\"\n","    Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n","    \n","for i in range(0, len(dataset2)):\n","    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n","    fileName = \"/content/drive/My Drive/MCA Dataset/300_P/\" + str(int(dataset2[i][0])) + \"_TRANSCRIPT.csv\"\n","    Data_test.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n","\n","Y = np.array(Y)\n","Data2 = []\n","\n","Data2_test = []\n","Y_test = np.array(Y_test)\n","\n","for i in range(0, len(Data)):\n","    script = []\n","    for k in range(1, len(Data[i])):\n","        if(Data[i][k][0] == \"Participant\"):\n","            script.append(Data[i][k][1])\n","    Data2.append(script)\n","    \n","for i in range(0, len(Data_test)):\n","    script = []\n","    for k in range(1, len(Data_test[i])):\n","        if(Data_test[i][k][0] == \"Participant\"):\n","            script.append(Data_test[i][k][1])\n","    Data2_test.append(script)\n","        \n","Data2 = np.array(Data2)\n","Data2_test = np.array(Data2_test)\n","\n","model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/MCA Dataset/GoogleNews-vectors-negative300.bin', binary=True)\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def remove_StopWOrds(sentence):\n","    filtered_sentence = [] \n","    for w in sentence: \n","        if w not in stop_words: \n","            filtered_sentence.append(w)\n","    \n","    return filtered_sentence\n","\n","# def checkAcc(Y_pred, Y_test):\n","#     correct = 0\n","#     for i in range(len(Y_pred)):\n","#         if(Y_pred[i] == Y_test[i]):\n","#             correct+=1\n","    \n","#     return float(correct)/len(Y_pred)\n","\n","max_num_words = 15\n","max_num_sentence = 200\n","\n","#train_data\n","finalMatrix = np.zeros((Data2.shape[0], max_num_sentence, max_num_words))\n","print(finalMatrix.shape)\n","for k in range(Data2.shape[0]):\n","    for i in range(min(max_num_sentence, len(Data2[k][0]))):\n","    \tsentence = Data2[k][i].split(\" \")\n","    \tsentence = remove_StopWOrds(sentence)\n","    \tfor j in range(min(max_num_words, len(sentence))):\n","    \t\ttry:\n","    \t\t\tfinalMatrix[k][i][j] = np.average(np.array(model[sentence[j]]))\n","    \t\texcept:\n","    \t\t\tcontinue\n","X = np.zeros((len(Data2), max_num_sentence*max_num_words))\n","for i in range(len(finalMatrix)):\n","    X[i] = finalMatrix[i].flatten().reshape(1, -1)\n"," \n","#Test_data\n","finalMatrix_test = np.zeros((Data2_test.shape[0], max_num_sentence, max_num_words))\n","print(finalMatrix_test.shape)\n","for k in range(Data2_test.shape[0]):\n","    for i in range(min(max_num_sentence, len(Data2_test[k][0]))):\n","    \tsentence = Data2_test[k][i].split(\" \")\n","    \tsentence = remove_StopWOrds(sentence)\n","    \tfor j in range(min(max_num_words, len(sentence))):\n","    \t\ttry:\n","    \t\t\tfinalMatrix_test[k][i][j] = np.average(np.array(model[sentence[j]]))\n","    \t\texcept:\n","    \t\t\tcontinue\n","X_test = np.zeros((len(Data2_test), max_num_sentence*max_num_words))\n","for i in range(len(finalMatrix_test)):\n","    X_test[i] = finalMatrix_test[i].flatten().reshape(1, -1)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-a442554b38a1>:89: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  Data2 = np.array(Data2)\n","<ipython-input-2-a442554b38a1>:90: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  Data2_test = np.array(Data2_test)\n"]},{"output_type":"stream","name":"stdout","text":["(52, 200, 15)\n","(25, 200, 15)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6FEUGw7JmHSE"},"source":["# Running Models"]},{"cell_type":"code","metadata":{"id":"Q9xt2TKulb8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681409882079,"user_tz":-330,"elapsed":822,"user":{"displayName":"Deepression SLP","userId":"18338550655322334499"}},"outputId":"d9e8c1ce-f8d7-4855-bebd-131ac3a1eb92"},"source":["import math\n","class RfText:\n","  def __init__(self):\n","    self.classifier = RandomForestRegressor()\n","  \n","  def fitModel(self, X, Y):\n","    self.classifier.fit(X, Y)\n","  \n","  def predictModel(self, X):\n","    return self.classifier.predict(X)\n","  \n","  def scoreModel(self, yt, yp):\n","    return math.sqrt(mean_squared_log_error(yt, yp))\n","\n","class SVMText:\n","  def __init__(self, kernel = \"rbf\"):\n","    self.classifier = SVR(kernel = kernel)\n","  \n","  def fitModel(self, X, Y):\n","    self.classifier.fit(X, Y)\n","  \n","  def predictModel(self, X):\n","    return self.classifier.predict(X)\n","  \n","  def scoreModel(self, yt, yp):\n","    return math.sqrt(mean_squared_log_error(yt, yp))\n","\n","\n","print(\"\\nRandom forest: \")\n","rfModel = RfText()\n","rfModel.fitModel(X, Y)\n","Y_pred1 = rfModel.predictModel(X_test)\n","print(rfModel.scoreModel(Y_test,Y_pred1))\n","# print(classification_report(Y_test,Y_pred1))\n","\n","print(\"\\nSVM: \")\n","svmModel = SVMText()\n","svmModel.fitModel(X, Y)\n","Y_pred2 = svmModel.predictModel(X_test)\n","print(svmModel.scoreModel(Y_test,Y_pred2))\n","# print(classification_report(Y_test, Y_pred2))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Random forest: \n","1.0437162188581561\n","\n","SVM: \n","0.9773470042609461\n"]}]}]}